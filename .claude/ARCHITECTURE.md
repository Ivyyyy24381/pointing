# ğŸ—ï¸ Project Architecture

**Clean, modular pipeline for pointing gesture analysis**

---

## ğŸ“ Design Principles

1. **Separation of Concerns**: Each step is independent and testable
2. **Single Responsibility**: One module = one clear purpose
3. **Data Flow**: Explicit inputs/outputs for each step
4. **Reusability**: Utility functions shared across steps
5. **Testability**: Each component has unit tests
6. **Documentation**: Every step has comprehensive README

---

## ğŸ“‚ Directory Structure

```
pointing/
â”‚
â”œâ”€â”€ step0_data_loading/              # Load and standardize input data
â”‚   â”œâ”€â”€ README.md                    # Data format and loading docs
â”‚   â”œâ”€â”€ data_loader.py               # Load images from trial folders
â”‚   â”œâ”€â”€ data_validator.py            # Validate data integrity
â”‚   â””â”€â”€ tests/                       # Unit tests
â”‚
â”œâ”€â”€ step1_calibration_process/       # Camera calibration
â”‚   â”œâ”€â”€ README.md                    # Calibration theory and usage
â”‚   â”œâ”€â”€ intrinsics.py                # Load/compute camera intrinsics
â”‚   â”œâ”€â”€ extrinsics.py                # Compute camera poses (AprilTag)
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ step2_target_detection/          # Detect and localize targets (cups)
â”‚   â”œâ”€â”€ README.md                    # Target detection methods
â”‚   â”œâ”€â”€ manual_labeling.py           # GUI for manual target labeling
â”‚   â”œâ”€â”€ projection.py                # 2D click â†’ 3D position
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ step3_skeleton_extraction/       # Detect human/animal skeleton
â”‚   â”œâ”€â”€ README.md                    # Skeleton detection docs
â”‚   â”œâ”€â”€ mediapipe_detector.py        # MediaPipe for humans/babies
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ step4_subject_extraction/        # Segment subject from background
â”‚   â”œâ”€â”€ README.md                    # Segmentation methods
â”‚   â”œâ”€â”€ sam2_segmenter.py            # SAM2 foreground segmentation
â”‚   â”œâ”€â”€ deeplabcut_detector.py       # DeepLabCut for dogs
â”‚   â”œâ”€â”€ mask_utils.py                # Mask processing utilities
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ step5_calculation/               # Compute metrics and analysis
â”‚   â”œâ”€â”€ README.md                    # Calculation methods
â”‚   â”œâ”€â”€ trajectory.py                # 3D trajectory computation
â”‚   â”œâ”€â”€ distances.py                 # Distance/angle calculations
â”‚   â”œâ”€â”€ pointing_inference.py        # Infer pointing direction
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ utils/                           # Shared utility functions
â”‚   â”œâ”€â”€ 2d_3d_conversion/           # Coordinate transformations
â”‚   â”‚   â”œâ”€â”€ projection.py            # 2D â†” 3D conversions
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ trial_data_structure_cleanup/  # Data cleaning utilities
â”‚   â”œâ”€â”€ visualization/               # Plotting and visualization
â”‚   â””â”€â”€ io/                          # File I/O helpers
â”‚
â”œâ”€â”€ web_ui/                          # Flask web interface
â”‚   â”œâ”€â”€ app.py                       # Main Flask app
â”‚   â”œâ”€â”€ templates/                   # HTML templates
â”‚   â”œâ”€â”€ static/                      # CSS/JS/images
â”‚   â””â”€â”€ README.md                    # Web UI documentation
â”‚
â”œâ”€â”€ trial_input/                     # Example input data
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ trial_output/                    # Processing outputs
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ legeacy_code/                    # Old implementation (reference only)
â”‚   â””â”€â”€ (archived code)
â”‚
â””â”€â”€ thirdparty/                      # External dependencies
    â””â”€â”€ (external libraries)
```

---

## ğŸ”„ Data Flow Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 0: Data Loading                                       â”‚
â”‚  Load images, validate format, standardize resolution       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ Color images (640Ã—480)
                     â”‚ Depth images (.npy or .png)
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Calibration                                         â”‚
â”‚  Load camera intrinsics (fx, fy, cx, cy)                   â”‚
â”‚  Optional: Compute extrinsics with AprilTag                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ Camera parameters
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Target Detection                                    â”‚
â”‚  User clicks on targets â†’ 2D pixels                         â”‚
â”‚  Project to 3D using depth + camera params                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ Target 3D positions [X, Y, Z]
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Skeleton Extraction                                 â”‚
â”‚  Detect keypoints: nose, wrists, shoulders, etc.           â”‚
â”‚  MediaPipe (humans) or DeepLabCut (dogs)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ 2D keypoints per frame
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: Subject Extraction (Optional)                       â”‚
â”‚  Segment subject from background using SAM2                 â”‚
â”‚  Improve skeleton detection accuracy                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ Segmentation masks
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: Calculation & Analysis                             â”‚
â”‚  Project 2D keypoints â†’ 3D positions                        â”‚
â”‚  Compute distances, angles, trajectories                    â”‚
â”‚  Infer pointing direction and target selection              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ Final results CSV + visualizations
                     â†“
                  OUTPUT
```

---

## ğŸ“Š Module Interfaces

### Step 0: Data Loading
```python
# Input
trial_dir: Path                    # e.g., "trial_input/trial_1/"

# Output
DataBatch {
    color_images: List[np.ndarray]      # (H, W, 3) uint8
    depth_images: List[np.ndarray]      # (H, W) float32 in meters
    frame_ids: List[int]
    metadata: Dict                       # Original resolution, etc.
}
```

### Step 1: Calibration
```python
# Input
config_file: Path                  # "config/camera_config.yaml"
# OR
calibration_images: List[np.ndarray]  # Images with AprilTag

# Output
CameraParams {
    fx, fy: float                      # Focal lengths
    cx, cy: float                      # Principal point
    width, height: int                 # Image dimensions
    distortion: Optional[np.ndarray]   # Distortion coefficients
}
```

### Step 2: Target Detection
```python
# Input
image: np.ndarray                  # Reference image
depth_image: np.ndarray
camera_params: CameraParams
user_clicks: List[Tuple[int, int]]  # [(x, y), ...]

# Output
List[Target] {
    id: int
    label: str                         # "target_1", "target_2", ...
    pixel_coords: Tuple[int, int]      # (x, y)
    world_coords: Tuple[float, float, float]  # (X, Y, Z)
    depth_m: float
}
```

### Step 3: Skeleton Extraction
```python
# Input
images: List[np.ndarray]
detector_type: str                 # "mediapipe" or "deeplabcut"

# Output
List[SkeletonFrame] {
    frame_id: int
    keypoints: Dict[str, Keypoint]     # name â†’ (x, y, confidence)
    bbox: Optional[Tuple[int, int, int, int]]  # (x, y, w, h)
}
```

### Step 4: Subject Extraction
```python
# Input
images: List[np.ndarray]
prompts: List[Point]               # Foreground/background points

# Output
List[np.ndarray]                   # Segmentation masks (H, W) bool
```

### Step 5: Calculation
```python
# Input
skeletons: List[SkeletonFrame]
depth_images: List[np.ndarray]
targets: List[Target]
camera_params: CameraParams

# Output
AnalysisResults {
    trajectory_3d: np.ndarray          # (N, 3) positions over time
    distances_to_targets: np.ndarray   # (N, num_targets)
    closest_target_per_frame: List[int]
    pointing_direction: Optional[np.ndarray]  # For human gestures
    orientation_vectors: np.ndarray    # Subject orientation
}
```

---

## ğŸ§© Utility Modules

### 2D â†” 3D Conversion
```python
from utils.projection import pixel_to_3d, project_3d_to_2d

# Pixel â†’ 3D
X, Y, Z = pixel_to_3d(x_px, y_px, depth_m, fx, fy, cx, cy)

# 3D â†’ Pixel
x_px, y_px = project_3d_to_2d(X, Y, Z, fx, fy, cx, cy)
```

### Visualization
```python
from utils.visualization import plot_trajectory_3d, plot_skeleton

plot_trajectory_3d(positions, targets, output_path="trajectory.png")
plot_skeleton(image, keypoints, connections)
```

### I/O
```python
from utils.io import load_depth, save_results

depth = load_depth("depth.npy")  # Auto-handles .npy, .png, .raw
save_results(results, "output/analysis.json")
```

---

## ğŸ§ª Testing Strategy

### Unit Tests
Each step has `tests/` folder with:
- `test_module.py` - Test individual functions
- `test_integration.py` - Test step end-to-end
- `test_data/` - Small sample datasets

### Running Tests
```bash
# Test specific step
pytest step1_calibration_process/tests/

# Test all
pytest

# With coverage
pytest --cov=. --cov-report=html
```

---

## ğŸ“ Documentation Standards

Each `README.md` includes:

1. **Purpose**: What does this step do?
2. **Theory**: Background/algorithms used
3. **Inputs**: Expected input format
4. **Outputs**: Output format and files
5. **Usage**: Command-line examples
6. **Parameters**: Configuration options
7. **Examples**: Code snippets
8. **Troubleshooting**: Common issues

---

## ğŸ”€ Branching Strategy

```
main                    # Stable, production-ready
â”œâ”€â”€ develop             # Integration branch
    â”œâ”€â”€ feature/step1-calibration
    â”œâ”€â”€ feature/step2-targets
    â”œâ”€â”€ feature/web-ui
    â””â”€â”€ bugfix/depth-loading
```

---

## ğŸš€ Development Workflow

### Adding a New Step

1. Create module structure:
   ```bash
   mkdir -p stepX_name/{tests,docs}
   touch stepX_name/{README.md,main.py,tests/test_main.py}
   ```

2. Write README with:
   - Purpose
   - Input/Output specs
   - Usage examples

3. Implement module with:
   - Clear function signatures
   - Type hints
   - Docstrings

4. Write tests:
   - Unit tests for each function
   - Integration test for full step

5. Update ARCHITECTURE.md

### Code Review Checklist

- [ ] Follows single responsibility principle
- [ ] Has comprehensive README
- [ ] Includes unit tests (>80% coverage)
- [ ] Type hints on all functions
- [ ] Docstrings for public API
- [ ] No hardcoded paths
- [ ] Error handling for edge cases
- [ ] Logging for debugging

---

## ğŸ¯ Current Status

| Step | Status | Completion |
|------|--------|------------|
| Step 0: Data Loading | âœ… Documented | 20% |
| Step 1: Calibration | ğŸ“ Planning | 0% |
| Step 2: Target Detection | ğŸ“ Planning | 0% |
| Step 3: Skeleton Extraction | ğŸ“ Planning | 0% |
| Step 4: Subject Extraction | ğŸ“ Planning | 0% |
| Step 5: Calculation | ğŸ“ Planning | 0% |
| Web UI | ğŸ“ Template | 5% |
| Utils | ğŸ”§ In Progress | 15% |

---

## ğŸ“§ Contributing

See individual step READMEs for implementation details.

When contributing:
1. Work on one step at a time
2. Write tests first (TDD)
3. Update documentation
4. Run full test suite before PR

---

**Last Updated**: 2025-01-09
**Version**: 2.0 (Refactored Architecture)
