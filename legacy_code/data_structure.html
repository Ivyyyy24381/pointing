<!doctype html>
<html>
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta http-equiv="X-UA-Compatible" content="ie=edge" />
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
html {
  font-family: ui-sans-serif, system-ui, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.markmap-dark {
  background: #27272a;
  color: white;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.12/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/index.js"></script><script>(r => {
              setTimeout(r);
            })(function renderToolbar() {
  const {
    markmap,
    mm
  } = window;
  const {
    el
  } = markmap.Toolbar.create(mm);
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root2, jsonOptions) => {
              const markmap = getMarkmap();
              window.mm = markmap.Markmap.create(
                "svg#mindmap",
                (getOptions || markmap.deriveOptions)(jsonOptions),
                root2
              );
              if (window.matchMedia("(prefers-color-scheme: dark)").matches) {
                document.documentElement.classList.add("markmap-dark");
              }
            })(() => window.markmap,null,{"content":"pointing data","children":[{"content":"pointing_production(front_view)","children":[{"content":"trials (e.g. 1)","children":[{"content":"Color","children":[{"content":"Color_%(FRAME_ID).png - RGB color frames extracted from rosbag (640x480 or 1280x720)","children":[],"payload":{"tag":"h5","lines":"10,11"}},{"content":"f_%(FRAME_ID).png - Alternative naming format for color frames","children":[],"payload":{"tag":"h5","lines":"11,12"}}],"payload":{"tag":"h4","lines":"9,10"}},{"content":"Depth","children":[{"content":"Depth_%(FRAME_ID).png - Depth visualization frames as PNG images","children":[],"payload":{"tag":"h5","lines":"13,14"}},{"content":"Depth_%(FRAME_ID).raw - Raw depth data in uint16 format (matching color frame dimensions)","children":[],"payload":{"tag":"h5","lines":"14,15"}}],"payload":{"tag":"h4","lines":"12,13"}},{"content":"Depth_Color","children":[{"content":"Depth_Color_%(FRAME_ID).png - Colorized depth images for visualization","children":[],"payload":{"tag":"h5","lines":"16,17"}}],"payload":{"tag":"h4","lines":"15,16"}},{"content":"Color.mp4 - Compiled color video from all trial frames","children":[],"payload":{"tag":"h4","lines":"17,18"}},{"content":"Depth.mp4 - Compiled depth video from all trial frames","children":[],"payload":{"tag":"h4","lines":"18,19"}},{"content":"Depth_Color.mp4 - Compiled colorized depth video","children":[],"payload":{"tag":"h4","lines":"19,20"}},{"content":"2d_pointing_trace.png - Visualization of detected 2D pointing gesture traces over time","children":[],"payload":{"tag":"h4","lines":"20,21"}},{"content":"gesture_data.csv - Raw gesture detection data with keypoint coordinates and confidence scores","children":[],"payload":{"tag":"h4","lines":"21,22"}},{"content":"processed_gesture_data.csv - Post-processed gesture data with smoothing and filtering applied","children":[],"payload":{"tag":"h4","lines":"22,23"}}],"payload":{"tag":"h3","lines":"8,9"}},{"content":"auto_split.csv - Automatic trial segmentation metadata with start/end timestamps","children":[],"payload":{"tag":"h3","lines":"23,24"}},{"content":"Color.mp4 - Full session color video before trial splitting","children":[],"payload":{"tag":"h3","lines":"24,25"}},{"content":"Depth.mp4 - Full session depth video before trial splitting","children":[],"payload":{"tag":"h3","lines":"25,26"}},{"content":"Depth_Color.mp4 - Full session colorized depth video before trial splitting","children":[],"payload":{"tag":"h3","lines":"26,27"}},{"content":"rosbag_metadata.yaml - Camera intrinsics and recording parameters extracted from rosbag","children":[],"payload":{"tag":"h3","lines":"27,28"}},{"content":"{subject_id}_front_gesture_data.csv - Combined gesture data across all trials for the subject","children":[],"payload":{"tag":"h3","lines":"28,29"}}],"payload":{"tag":"h2","lines":"7,8"}},{"content":"pointing_comprehension(side_view)","children":[{"content":"trials (e.g. 1)","children":[{"content":"Color","children":[{"content":"Color_%(FRAME_ID).png - RGB color frames standardized to 1280x720 to match camera intrinsics","children":[],"payload":{"tag":"h5","lines":"33,34"}}],"payload":{"tag":"h4","lines":"32,33"}},{"content":"Depth","children":[{"content":"Depth_%(FRAME_ID).png - Depth visualization frames standardized to 1280x720","children":[],"payload":{"tag":"h5","lines":"35,36"}},{"content":"Depth_%(FRAME_ID).raw - Raw uint16 depth data standardized to 1280x720 dimensions","children":[],"payload":{"tag":"h5","lines":"36,37"}}],"payload":{"tag":"h4","lines":"34,35"}},{"content":"Depth_Color","children":[{"content":"Depth_Color_%(FRAME_ID).png - Colorized depth images standardized to 1280x720","children":[],"payload":{"tag":"h5","lines":"38,39"}}],"payload":{"tag":"h4","lines":"37,38"}},{"content":"segmented_color - Directory containing SAM2 segmentation visualization frames","children":[],"payload":{"tag":"h4","lines":"39,40"}},{"content":"Color.mp4 - Trial color video compiled from standardized frames at original framerate (15-25 FPS)","children":[],"payload":{"tag":"h4","lines":"40,41"}},{"content":"Depth.mp4 - Trial depth video compiled from standardized frames","children":[],"payload":{"tag":"h4","lines":"41,42"}},{"content":"Depth_Color.mp4 - Trial colorized depth video compiled from standardized frames","children":[],"payload":{"tag":"h4","lines":"42,43"}},{"content":"masked_video.mp4 - SAM2-segmented video with subject mask applied (10-12.5 FPS after sampling)","children":[],"payload":{"tag":"h4","lines":"43,44"}},{"content":"masked_video_annotated.mp4 - SAM2 segmentation with visual overlays showing mask boundaries","children":[],"payload":{"tag":"h4","lines":"44,45"}},{"content":"subject_annotated_video.mp4 - Final output with pose skeleton, bounding boxes, and target distance annotations (10-12.5 FPS)","children":[],"payload":{"tag":"h4","lines":"45,46"}},{"content":"masked_video_skeleton.json - MediaPipe pose detection results for each frame with bodypart coordinates and confidence scores","children":[],"payload":{"tag":"h4","lines":"46,47"}},{"content":"interactive_points.json - User-labeled foreground/background points for SAM2 segmentation training","children":[],"payload":{"tag":"h4","lines":"47,48"}},{"content":"target_coordinates.json - 3D world coordinates of reference targets for distance calculation","children":[],"payload":{"tag":"h4","lines":"48,49"}},{"content":"sam2_scale_metadata.json - Scale factors and processing metadata from SAM2 segmentation pipeline","children":[],"payload":{"tag":"h4","lines":"49,50"}},{"content":"processed_subject_result_distance_comparison.png - Plot showing distance to each target over time","children":[],"payload":{"tag":"h4","lines":"50,51"}},{"content":"processed_subject_result_trace.png - 2D trajectory plot of subject center-of-mass movement","children":[],"payload":{"tag":"h4","lines":"51,52"}},{"content":"processed_subject_result_trace3d.png - 3D trajectory visualization of subject movement in world coordinates","children":[],"payload":{"tag":"h4","lines":"52,53"}},{"content":"processed_subject_result_table.csv - Comprehensive frame-by-frame data including:","children":[{"content":"Frame timing (frame_index, time_sec, local_frame_index)","children":[],"payload":{"tag":"li","lines":"54,55"}},{"content":"Target distances (human_r, target_N_r) and angles (theta, phi)","children":[],"payload":{"tag":"li","lines":"55,56"}},{"content":"Bounding box coordinates (bbox_x, bbox_y, bbox_w, bbox_h)","children":[],"payload":{"tag":"li","lines":"56,57"}},{"content":"All MediaPipe keypoint coordinates in 2D pixels and 3D world meters","children":[],"payload":{"tag":"li","lines":"57,58"}},{"content":"Confidence scores for each detected keypoint","children":[],"payload":{"tag":"li","lines":"58,59"}},{"content":"Subject orientation vectors (head_orientation, torso_orientation)","children":[],"payload":{"tag":"li","lines":"59,60"}},{"content":"3D center-of-mass trajectory (trace3d_x, trace3d_y, trace3d_z)","children":[],"payload":{"tag":"li","lines":"60,61"}}],"payload":{"tag":"h4","lines":"53,54"}}],"payload":{"tag":"h3","lines":"31,32"}},{"content":"auto_split.csv - Automatic trial segmentation metadata with start/end timestamps","children":[],"payload":{"tag":"h3","lines":"61,62"}},{"content":"Color.mp4 - Full session color video before trial splitting","children":[],"payload":{"tag":"h3","lines":"62,63"}},{"content":"Depth.mp4 - Full session depth video before trial splitting","children":[],"payload":{"tag":"h3","lines":"63,64"}},{"content":"Depth_Color.mp4 - Full session colorized depth video before trial splitting","children":[],"payload":{"tag":"h3","lines":"64,65"}},{"content":"rosbag_metadata.yaml - Camera intrinsics (fx, fy, ppx, ppy, width, height) and recording parameters","children":[],"payload":{"tag":"h3","lines":"65,66"}},{"content":"{subject_id}_side_combined_result.csv - Aggregated results across all trials for comprehensive analysis","children":[],"payload":{"tag":"h3","lines":"66,67"}}],"payload":{"tag":"h2","lines":"30,31"}},{"content":"File Processing Pipeline","children":[{"content":"Stage 1: Rosbag Processing","children":[{"content":"Extract rosbag_metadata.yaml (camera intrinsics, recording metadata)","children":[],"payload":{"tag":"li","lines":"70,71"}},{"content":"Generate Color.mp4, Depth.mp4, Depth_Color.mp4 (full session videos)","children":[],"payload":{"tag":"li","lines":"71,72"}},{"content":"Create auto_split.csv (trial boundary detection)","children":[],"payload":{"tag":"li","lines":"72,73"}},{"content":"Extract individual frame images to Color/, Depth/, Depth_Color/ directories","children":[],"payload":{"tag":"li","lines":"73,75"}}],"payload":{"tag":"h3","lines":"69,70"}},{"content":"Stage 2: Image Standardization","children":[{"content":"Resize all color/depth images to 1280x720 to match camera intrinsics","children":[],"payload":{"tag":"li","lines":"76,77"}},{"content":"Maintain temporal alignment and preserve depth data accuracy","children":[],"payload":{"tag":"li","lines":"77,78"}},{"content":"Update metadata to reflect new dimensions","children":[],"payload":{"tag":"li","lines":"78,80"}}],"payload":{"tag":"h3","lines":"75,76"}},{"content":"Stage 3: SAM2 Segmentation (side_view only)","children":[{"content":"Load interactive_points.json for user-labeled training points","children":[],"payload":{"tag":"li","lines":"81,82"}},{"content":"Apply frame sampling (every 2-3 frames) for memory management","children":[],"payload":{"tag":"li","lines":"82,83"}},{"content":"Generate masked_video.mp4 and masked_video_annotated.mp4","children":[],"payload":{"tag":"li","lines":"83,84"}},{"content":"Save sam2_scale_metadata.json with processing parameters","children":[],"payload":{"tag":"li","lines":"84,86"}}],"payload":{"tag":"h3","lines":"80,81"}},{"content":"Stage 4: Pose Detection (side_view only)","children":[{"content":"Run MediaPipe pose detection on masked_video.mp4","children":[],"payload":{"tag":"li","lines":"87,88"}},{"content":"Generate masked_video_skeleton.json with keypoint coordinates","children":[],"payload":{"tag":"li","lines":"88,89"}},{"content":"Apply coordinate corrections based on scale metadata","children":[],"payload":{"tag":"li","lines":"89,91"}}],"payload":{"tag":"h3","lines":"86,87"}},{"content":"Stage 5: 3D Analysis (side_view only)","children":[{"content":"Convert 2D keypoints to 3D world coordinates using camera intrinsics","children":[],"payload":{"tag":"li","lines":"92,93"}},{"content":"Calculate distances to reference targets from target_coordinates.json","children":[],"payload":{"tag":"li","lines":"93,94"}},{"content":"Generate trajectory plots and distance comparison visualizations","children":[],"payload":{"tag":"li","lines":"94,95"}},{"content":"Create comprehensive processed_subject_result_table.csv","children":[],"payload":{"tag":"li","lines":"95,96"}},{"content":"Produce subject_annotated_video.mp4 with all annotations overlaid","children":[],"payload":{"tag":"li","lines":"96,97"}}],"payload":{"tag":"h3","lines":"91,92"}}],"payload":{"tag":"h2","lines":"68,69"}}],"payload":{"tag":"h1","lines":"6,7"}},{"colorFreezeLevel":3})</script>
</body>
</html>
