# Output Data Reference

This document describes all output files generated by the batch processing pipeline.

## Output Directory Structure

```
<study_name>_output/
  trial_N/
    cam1/
      # Target Detection
      target_detections_cam_frame.json
      ground_plane_transform.json

      # Human Skeleton & Pointing Analysis
      skeleton_2d.json
      pointing_hand.json
      processed_gesture.csv
      2d_pointing_trace.png
      human_center.json
      detection_summary.txt
      sample_verification_3d.png

      # Distance Analysis
      distance_to_targets_timeseries.png
      distance_to_targets_summary.png
      pointing_accuracy_comparison.png

      # Subject Detection (dog/baby)
      dog_detection_results.json
      processed_dog_result_table.csv
      dog_result_trace2d.png
      dog_distance_to_targets.png
      sample_dog_overlay.png

    cam2/
      ...

    # Trial-level summaries
    trial_summary.json
    cam1_dog_trace_from_json.png
    cam1_pointing_distance_to_targets_from_json.png
```

---

## Coordinate System Reference

**Camera Frame (all 3D data uses this):**
- **+X**: Camera's right (human's left when facing camera)
- **+Y**: Down (toward floor)
- **+Z**: Into scene (depth, away from camera)

**Experiment Setup:**
- Targets arranged in a **curved arc**
- Target 1 & 2 (+X): Human's LEFT
- Target 3 & 4 (-X): Human's RIGHT
- Dog: CENTER of arc (inside the curve)
- Human: OUTSIDE of curve (pointing toward targets/dog)

---

## File Descriptions

### Target Detection

#### `target_detections_cam_frame.json`
**Purpose:** 3D positions of detected target cups in camera frame.

```json
[
  {
    "bbox": [x1, y1, x2, y2],        // 2D bounding box in pixels
    "center_px": [cx, cy],            // 2D center in pixels
    "avg_depth_m": 2.85,              // Average depth within bbox (meters)
    "x": 1.16,                        // 3D X position (meters)
    "y": 0.82,                        // 3D Y position (meters)
    "z": 2.62,                        // 3D Z position / depth (meters)
    "label": "target_1"               // Target ID (sorted right-to-left)
  },
  ...
]
```

**Target ordering:** Sorted by X coordinate (right to left in camera view)
- `target_1`: Rightmost (+X, human's left)
- `target_4`: Leftmost (-X, human's right)

---

#### `ground_plane_transform.json`
**Purpose:** Rotation matrix to correct camera tilt.

```json
{
  "rotation_matrix": [[...], [...], [...]],  // 3x3 rotation matrix
  "transform_type": "tilt_only",              // "tilt_only" or "full_ground_plane"
  "info": {
    "angle_deg": 5.2,                         // Tilt angle in degrees
    "plane_normal": [nx, ny, nz]              // Fitted ground plane normal
  },
  "description": "..."
}
```

**Transform types:**
- `tilt_only`: Only corrects camera pitch (preserves arc shape)
- `full_ground_plane`: Full plane alignment (may distort arc arrangements)

---

### Human Skeleton & Pointing Analysis

#### `skeleton_2d.json`
**Purpose:** Per-frame human pose detection results.

```json
{
  "frame_000001": {
    "landmarks_2d": [[x, y], ...],           // 33 MediaPipe landmarks (pixels)
    "landmarks_3d": [[x, y, z], ...],        // 33 landmarks in 3D (meters)
    "arm_vectors": {
      "eye_to_wrist": [vx, vy, vz],          // Pointing vector (normalized)
      "shoulder_to_wrist": [...],
      "elbow_to_wrist": [...],
      "nose_to_wrist": [...],
      "wrist_location": [x, y, z]            // 3D wrist position
    },
    "metadata": {
      "pointing_arm": "right",               // Detected pointing arm
      "pointing_hand_whole_trial": "right"   // Trial-level consensus
    }
  },
  ...
}
```

**MediaPipe landmark indices:**
| Index | Landmark | Index | Landmark |
|-------|----------|-------|----------|
| 0 | Nose | 15 | Left wrist |
| 2 | Left eye inner | 16 | Right wrist |
| 5 | Right eye inner | 23 | Left hip |
| 11 | Left shoulder | 24 | Right hip |
| 12 | Right shoulder | 25 | Left knee |
| 13 | Left elbow | 26 | Right knee |
| 14 | Right elbow | 27-28 | Ankles |

---

#### `pointing_hand.json`
**Purpose:** Summary of pointing hand detection.

```json
{
  "pointing_hand": "right",          // Dominant pointing hand for trial
  "total_frames": 150,               // Frames with skeleton detection
  "frame_distribution": {
    "right": 145,                    // Frames pointing with right hand
    "left": 3,                       // Frames pointing with left hand
    "unknown": 2                     // Frames with unclear pointing
  }
}
```

---

#### `processed_gesture.csv`
**Purpose:** Main analysis output - pointing vectors, ground intersections, and distances to targets.

| Column | Description | Units |
|--------|-------------|-------|
| `frame` | Frame number | - |
| `timestamp` | Time from start | seconds |
| `wrist_x`, `wrist_y`, `wrist_z` | Wrist 3D position | meters |
| `eye_to_wrist_ground_intersection_x/y/z` | Where eye-wrist ray hits ground | meters |
| `eye_to_wrist_dist_to_target_1` | Distance from intersection to target 1 | meters |
| `eye_to_wrist_dist_to_target_2` | Distance to target 2 | meters |
| `eye_to_wrist_dist_to_target_3` | Distance to target 3 | meters |
| `eye_to_wrist_dist_to_target_4` | Distance to target 4 | meters |
| `shoulder_to_wrist_ground_intersection_x/y/z` | Shoulder-wrist ray intersection | meters |
| `shoulder_to_wrist_dist_to_target_1-4` | Distances for shoulder-wrist | meters |
| `elbow_to_wrist_*` | Same for elbow-wrist vector | meters |
| `nose_to_wrist_*` | Same for nose-wrist vector | meters |
| `head_orientation_*` | Head gaze direction analysis | meters |
| `outliers_filtered` | Number of invalid intersections filtered | count |

**Pointing vector representations:**
1. **eye_to_wrist**: Line from eye center to wrist (most intuitive)
2. **shoulder_to_wrist**: Line from shoulder to wrist (arm extension)
3. **elbow_to_wrist**: Line from elbow to wrist (forearm direction)
4. **nose_to_wrist**: Line from nose to wrist (head-hand alignment)
5. **head_orientation**: Where the person is looking (gaze direction)

---

#### `2d_pointing_trace.png`
**Purpose:** Visualization of pointing intersection points over time.

**What it shows:**
- Black X markers: Target positions
- Gray circle: Human center position
- Colored dots: Ground intersection points for each pointing vector
  - Red: eye_to_wrist
  - Green: shoulder_to_wrist
  - Blue: elbow_to_wrist
  - Magenta: nose_to_wrist
- Alpha (transparency) increases over time (faint = early, solid = late)

**Axes:**
- X-axis: Left-right position (meters)
- Y-axis: Depth / Z position (meters, into scene)

---

#### `human_center.json`
**Purpose:** Estimated human center position (for visualizations).

```json
{
  "human_center": [x, y, z],              // Average hip center position
  "hip_detected": true,                    // Whether hip landmarks were found
  "num_hip_frames": 148,                   // Frames with valid hip detection
  "human_center_ground_aligned": [x, y, z] // Position after ground rotation
}
```

---

#### `detection_summary.txt`
**Purpose:** Human-readable summary of detection results.

```
Detection Summary
========================================
Trial: trial_6
Camera: cam1
Human frames: 150
Pointing hand: right

Per-frame arm distribution:
  right: 145 (96.7%)
  left: 3 (2.0%)
  unknown: 2 (1.3%)
```

---

#### `sample_verification_3d.png`
**Purpose:** 3D visualization for verifying coordinate transforms are correct.

**What it shows:**
- Blue skeleton with joint connections
- Gold stars: Target positions
- Green transparent surface: Ground plane
- Colored dashed lines: Pointing rays from wrist to ground
- X markers: Where rays intersect ground

---

### Distance Analysis Plots

#### `distance_to_targets_timeseries.png`
**Purpose:** How pointing accuracy changes over time.

**What it shows:**
- X-axis: Frame number
- Y-axis: Distance from pointing intersection to each target (meters)
- One subplot per target
- Different colors for each pointing vector type
- Lower values = more accurate pointing

---

#### `distance_to_targets_summary.png`
**Purpose:** Overall pointing accuracy summary.

**What it shows:**
- Bar chart comparing mean distance to each target
- Grouped by pointing vector type
- Error bars show standard deviation
- Helps identify which vector representation is most accurate

---

#### `pointing_accuracy_comparison.png`
**Purpose:** Heatmap showing which pointing vector works best for each target.

**What it shows:**
- Rows: Pointing vector types
- Columns: Targets
- Color intensity: Mean distance (darker = closer/better)
- Helps choose the best pointing representation for analysis

---

### Subject (Dog/Baby) Detection

#### `dog_detection_results.json` / `baby_detection_results.json`
**Purpose:** Per-frame subject detection results.

```json
{
  "frame_000001": {
    "subject_type": "dog",
    "bbox": [x1, y1, x2, y2],            // 2D bounding box (pixels)
    "detection_region": "bottom_half",    // Where detection was searched
    "keypoints_2d": [[cx, cy]],          // 2D center point (pixels)
    "keypoints_3d": [[x, y, z]]          // 3D center position (meters)
  },
  ...
}
```

---

#### `processed_dog_result_table.csv` / `processed_baby_result_table.csv`
**Purpose:** Subject tracking data with distances to targets.

| Column | Description | Units |
|--------|-------------|-------|
| `frame` | Frame number | - |
| `timestamp` | Time from start | seconds |
| `dog_x`, `dog_y`, `dog_z` | Subject 3D position | meters |
| `dog_dist_to_target_1` | Distance to target 1 | meters |
| `dog_dist_to_target_2` | Distance to target 2 | meters |
| `dog_dist_to_target_3` | Distance to target 3 | meters |
| `dog_dist_to_target_4` | Distance to target 4 | meters |
| `human_wrist_x/y/z` | Human wrist position (if available) | meters |

---

#### `dog_result_trace2d.png` / `baby_result_trace2d.png`
**Purpose:** Top-down view of subject movement over time.

**What it shows:**
- Gray squares: Target positions
- Rainbow-colored path: Subject movement trace
  - Blue/purple: Start of trial
  - Red/orange: End of trial
- Smooth spline fit through detection points

---

#### `dog_distance_to_targets.png` / `baby_distance_to_targets.png`
**Purpose:** Subject distance to each target over time.

**What it shows:**
- X-axis: Frame number
- Y-axis: Distance to targets (meters)
- Different colored lines for each target
- Useful for analyzing approach behavior

---

#### `sample_dog_overlay.png` / `sample_baby_overlay.png`
**Purpose:** Visual verification of detection quality.

**What it shows:**
- Original color frame (middle of trial)
- Green box: Subject bounding box
- Blue skeleton: Human pose overlay
- Gold stars: Target cup positions

---

### Trial-Level Outputs

#### `trial_summary.json`
**Purpose:** Summary of all cameras processed for this trial.

```json
{
  "trial": "trial_6",
  "cameras": ["cam1", "cam2"],
  "subject_type": "dog",
  "cam1": {
    "gesture_csv": "/path/to/processed_gesture.csv",
    "subject_json": "/path/to/dog_detection_results.json",
    "pointing_hand": {
      "pointing_hand": "right",
      "total_frames": 150
    }
  }
}
```

---

#### `cam1_dog_trace_from_json.png`
**Purpose:** Subject trace regenerated from saved JSON (verifies data integrity).

Same as `dog_result_trace2d.png` but explicitly loaded from JSON files.

---

#### `cam1_pointing_distance_to_targets_from_json.png`
**Purpose:** Distance plot regenerated from saved CSV (verifies data integrity).

Same as distance timeseries but explicitly loaded from exported CSV.

---

## Data Processing Notes

### Kalman Filtering
All 3D trajectories are smoothed using Kalman filtering:
- **Landmark-level**: Applied during detection (process_noise=0.005, measurement_noise=0.05)
- **Trajectory-level**: Applied after analysis (process_noise=0.01, measurement_noise=0.1)

### Outlier Filtering
Ground intersections are filtered if:
- Z (depth) < 1.5m or > 5.0m
- X (lateral) < -2.0m or > 2.0m
- Distance to nearest target > 2.0m
- Intersection is behind the person (Z < wrist_Z - 1.0m)

### Ground Plane Correction
- **Arc arrangement detected**: Uses tilt-only correction (preserves X-Z shape)
- **Flat arrangement**: Uses full ground plane rotation
- **Skipped**: Use `--skip-ground-rotation` flag for raw camera coordinates

---

## Quick Reference: Key Files for Analysis

| Task | File(s) |
|------|---------|
| Human pointing analysis | `processed_gesture.csv` |
| Subject (dog/baby) tracking | `processed_dog_result_table.csv` |
| Visual verification | `2d_pointing_trace.png`, `sample_verification_3d.png` |
| Comparing pointing methods | `pointing_accuracy_comparison.png` |
| Subject movement visualization | `dog_result_trace2d.png` |
| Raw detection data | `skeleton_2d.json`, `dog_detection_results.json` |

---

## Regenerating Visualizations

Use `batch_regenerate_plots.py` to regenerate plots from existing data with different settings:

```bash
# Regenerate all plots with default settings
python batch_regenerate_plots.py /path/to/study_output

# Custom axis ranges
python batch_regenerate_plots.py /path/to/study_output --z-min 2.0 --z-max 5.0

# Only pointing trace plots
python batch_regenerate_plots.py /path/to/study_output --only pointing

# Only subject (dog) trace plots
python batch_regenerate_plots.py /path/to/study_output --only subject

# Specific trial and camera
python batch_regenerate_plots.py /path/to/study_output --trial trial_6 --camera cam1

# All options
python batch_regenerate_plots.py /path/to/study_output \
    --x-min -2.0 --x-max 2.0 \
    --z-min 1.5 --z-max 5.0 \
    --subject dog \
    --only pointing subject distance
```

**Available plot types:**
- `pointing` - 2D pointing trace (human)
- `subject` - Subject (dog/baby) movement trace
- `distance` - Distance to targets analysis plots
- `subject_distance` - Subject distance to targets over time
- `all` - All of the above

---

## Reprocessing with Arm Override

If the automatic pointing arm detection is incorrect for some trials, you can reprocess with manual arm overrides using a CSV file.

### Step 1: Generate CSV Template

```bash
python batch_process_study.py /path/to/study_folder --generate-arm-csv
```

This creates `<study_name>_arm_overrides.csv` with columns:

| Column | Description |
|--------|-------------|
| `study` | Study/bag name |
| `trial` | Trial name (e.g., trial_1) |
| `camera` | Camera name (e.g., cam1) |
| `detected_arm` | Auto-detected pointing arm |
| `override_arm` | **Edit this**: `left`, `right`, or `skip` |
| `reprocess` | **Edit this**: `yes` to reprocess, `no` to skip |

### Step 2: Edit the CSV

Open the CSV in Excel or any text editor and:
1. Review the `detected_arm` column
2. Set `override_arm` to `left` or `right` where the detection is wrong
3. Set `reprocess` to `yes` for trials you want to reprocess
4. Set `override_arm` to `skip` to skip a trial entirely

Example CSV:
```csv
study,trial,camera,detected_arm,override_arm,reprocess
BDL396_study1,trial_1,cam1,right,,no
BDL396_study1,trial_2,cam1,left,right,yes
BDL396_study1,trial_3,cam1,unknown,left,yes
BDL396_study1,trial_4,cam1,right,skip,no
```

### Step 3: Reprocess with Pointing-Only Mode

```bash
# Reprocess only pointing analysis (fast - no skeleton re-detection)
python batch_process_study.py /path/to/study_folder --pointing-only --arm-csv arms.csv

# Reprocess specific trial
python batch_process_study.py /path/to/study_folder --pointing-only --arm-csv arms.csv --trial trial_2

# Full reprocess with arm override (slower - re-runs skeleton detection)
python batch_process_study.py /path/to/study_folder --arm-csv arms.csv
```

**Options:**
- `--pointing-only`: Skip skeleton detection, only reprocess pointing analysis (much faster)
- `--arm-csv <path>`: Path to arm override CSV file
- `--trial <name>`: Process only specific trial
- `--cameras <cam1 cam2>`: Process only specific cameras
